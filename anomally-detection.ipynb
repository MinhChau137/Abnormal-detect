{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d0140ea",
   "metadata": {
    "papermill": {
     "duration": 0.011011,
     "end_time": "2023-04-01T23:59:55.262738",
     "exception": false,
     "start_time": "2023-04-01T23:59:55.251727",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Problem Breakdown\n",
    "\n",
    "### Problem Statement: \n",
    "XYZ Company is a manufacturer of household cleaning products, operating across four different countries with an annual revenue of USD 800 million. Their manufacturing plant relies on over 30 industrial pumps to produce their range of detergents, shampoos, and other cleaning supplies. Recently, one of their pumps has failed unexpectedly seven times in the last six months, causing significant production losses estimated at USD 500,000, in addition to environmental damage caused by spilled toxic chemicals amounting to USD 300,000. To avoid such problems in the future, XYZ Company is keen to identify anomalies in pump behavior and take corrective action before the pump fails. By implementing a system that can detect and flag unusual pump behavior, the company aims to prevent any hard failures and minimize production losses and environmental damage.\n",
    "\n",
    "\n",
    "### Problem Context: \n",
    "Manufacturing industry relies on heavy machinery, including motors, pumps, pipes, furnaces, and conveyors. Asset Management programs prioritize equipment integrity and reliability to avoid production losses, which can result in financial losses of hundreds of thousands or millions of dollars. Robust Asset Management frameworks, including skilled Reliability Engineers, can detect anomalies and prevent unplanned downtime, unnecessary maintenance, and critical component shortages. These prevention measures can save manufacturing plants significant financial losses due to unplanned downtime, maintenance costs, and excess or shortage of critical components\n",
    "\n",
    "### Solution: \n",
    "Model with good accuracy score with zero rework\n",
    "\n",
    "\n",
    "### Scope \n",
    "This project focuses solely on identifying anomalies in the 53 sensors of the chosen pump and does not encompass other pumps. It also does not involve predicting potential pump failures.\n",
    "\n",
    "\n",
    "### Solution \n",
    "My approach involves creating a benchmark model using the IQR technique, and then implementing two other unsupervised learning algorithms to compare their performances and accuracies. To achieve this, I will undertake the following steps:\n",
    "\n",
    "- Source and load the data \n",
    "- Perform data wrangling\n",
    "- Conduct Exploratory Data Analysis (EDA)\n",
    "- Carry out pre-processing and feature engineering\n",
    "- Develop models: IQR and K-Means\n",
    "- Evaluate the models: \n",
    "\n",
    "### Result of Analysis: \n",
    "Kmeans comes on top v/s IQR model with accuracy of 87.7% v/s 86.6%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861328a1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-01T23:59:55.285862Z",
     "iopub.status.busy": "2023-04-01T23:59:55.285404Z",
     "iopub.status.idle": "2023-04-01T23:59:55.304580Z",
     "shell.execute_reply": "2023-04-01T23:59:55.302845Z"
    },
    "papermill": {
     "duration": 0.03481,
     "end_time": "2023-04-01T23:59:55.307637",
     "exception": false,
     "start_time": "2023-04-01T23:59:55.272827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beaa013",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-01T23:59:55.331096Z",
     "iopub.status.busy": "2023-04-01T23:59:55.330418Z",
     "iopub.status.idle": "2023-04-02T00:00:09.494765Z",
     "shell.execute_reply": "2023-04-02T00:00:09.493682Z"
    },
    "papermill": {
     "duration": 14.179389,
     "end_time": "2023-04-02T00:00:09.497808",
     "exception": false,
     "start_time": "2023-04-01T23:59:55.318419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas_profiling as pp\n",
    "from pandas_profiling import ProfileReport\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import random\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d78569a",
   "metadata": {
    "papermill": {
     "duration": 0.009903,
     "end_time": "2023-04-02T00:00:09.519267",
     "exception": false,
     "start_time": "2023-04-02T00:00:09.509364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 2 : Data Wrangling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d6fb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sensor.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c08a364",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:00:12.754455Z",
     "iopub.status.busy": "2023-04-02T00:00:12.754144Z",
     "iopub.status.idle": "2023-04-02T00:00:12.763043Z",
     "shell.execute_reply": "2023-04-02T00:00:12.761817Z"
    },
    "papermill": {
     "duration": 0.023595,
     "end_time": "2023-04-02T00:00:12.765667",
     "exception": false,
     "start_time": "2023-04-02T00:00:12.742072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce962556",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:00:12.789040Z",
     "iopub.status.busy": "2023-04-02T00:00:12.788692Z",
     "iopub.status.idle": "2023-04-02T00:00:12.845008Z",
     "shell.execute_reply": "2023-04-02T00:00:12.843325Z"
    },
    "papermill": {
     "duration": 0.071194,
     "end_time": "2023-04-02T00:00:12.847871",
     "exception": false,
     "start_time": "2023-04-02T00:00:12.776677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's get basic info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000de8d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:00:12.871912Z",
     "iopub.status.busy": "2023-04-02T00:00:12.871538Z",
     "iopub.status.idle": "2023-04-02T00:00:13.263166Z",
     "shell.execute_reply": "2023-04-02T00:00:13.262170Z"
    },
    "papermill": {
     "duration": 0.405859,
     "end_time": "2023-04-02T00:00:13.265200",
     "exception": false,
     "start_time": "2023-04-02T00:00:12.859341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ea2709",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:00:13.288673Z",
     "iopub.status.busy": "2023-04-02T00:00:13.288276Z",
     "iopub.status.idle": "2023-04-02T00:00:13.308086Z",
     "shell.execute_reply": "2023-04-02T00:00:13.306730Z"
    },
    "papermill": {
     "duration": 0.035222,
     "end_time": "2023-04-02T00:00:13.311240",
     "exception": false,
     "start_time": "2023-04-02T00:00:13.276018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's count the unique values of one of the columns\n",
    "df['machine_status'].value_counts(dropna=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e1717e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:00:13.335237Z",
     "iopub.status.busy": "2023-04-02T00:00:13.334843Z",
     "iopub.status.idle": "2023-04-02T00:00:20.037933Z",
     "shell.execute_reply": "2023-04-02T00:00:20.036762Z"
    },
    "papermill": {
     "duration": 6.718803,
     "end_time": "2023-04-02T00:00:20.041143",
     "exception": false,
     "start_time": "2023-04-02T00:00:13.322340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Histogram\n",
    "df.hist(df.columns, bins=25, layout=(8,7), figsize=(20, 18))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1544d5f",
   "metadata": {
    "papermill": {
     "duration": 0.013416,
     "end_time": "2023-04-02T00:00:20.068817",
     "exception": false,
     "start_time": "2023-04-02T00:00:20.055401",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Clean the data\n",
    "- Remove redundant columns\n",
    "- Remove duplicates\n",
    "- Handle missing values\n",
    "- Convert data types to the correct data type\n",
    "Keep in mind: there are data formats that are better for reporting vs. better for analysis; tidy data makes it easier to fix common data problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07ff43f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:00:20.098623Z",
     "iopub.status.busy": "2023-04-02T00:00:20.097296Z",
     "iopub.status.idle": "2023-04-02T00:00:20.848269Z",
     "shell.execute_reply": "2023-04-02T00:00:20.847120Z"
    },
    "papermill": {
     "duration": 0.768653,
     "end_time": "2023-04-02T00:00:20.850788",
     "exception": false,
     "start_time": "2023-04-02T00:00:20.082135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df = df.drop_duplicates()\n",
    "# Entire \"sensor_15\" column is NaN therefore removing the entire column from the data set\n",
    "del df['sensor_15']\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a6c90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:00:20.882555Z",
     "iopub.status.busy": "2023-04-02T00:00:20.880014Z",
     "iopub.status.idle": "2023-04-02T00:00:20.940358Z",
     "shell.execute_reply": "2023-04-02T00:00:20.939266Z"
    },
    "papermill": {
     "duration": 0.077208,
     "end_time": "2023-04-02T00:00:20.942339",
     "exception": false,
     "start_time": "2023-04-02T00:00:20.865131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function that calculates the percentage of missing values\n",
    "def calc_percent_NAs(df):\n",
    "    nans = pd.DataFrame(df.isnull().sum().sort_values(ascending=False)/len(df), columns=['percent']) \n",
    "    idx = nans['percent'] > 0\n",
    "    return nans[idx]\n",
    "\n",
    "# Let's take a look at top ten columns with missing values\n",
    "calc_percent_NAs(df).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31701006",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:00:20.972290Z",
     "iopub.status.busy": "2023-04-02T00:00:20.971891Z",
     "iopub.status.idle": "2023-04-02T00:00:20.987664Z",
     "shell.execute_reply": "2023-04-02T00:00:20.986651Z"
    },
    "papermill": {
     "duration": 0.033992,
     "end_time": "2023-04-02T00:00:20.990076",
     "exception": false,
     "start_time": "2023-04-02T00:00:20.956084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Lets fill Let's fill NAs in columns sensor_50, sensor_51, sensor_00, sensor_07, sensor_08, sensor_06 and sensor_09 with their mean values\n",
    "\n",
    "df['sensor_50'].fillna((df['sensor_50'].mean()), inplace=True)\n",
    "df['sensor_51'].fillna((df['sensor_51'].mean()), inplace=True)\n",
    "df['sensor_00'].fillna((df['sensor_00'].mean()), inplace=True)\n",
    "df['sensor_08'].fillna((df['sensor_08'].mean()), inplace=True)\n",
    "df['sensor_07'].fillna((df['sensor_07'].mean()), inplace=True)\n",
    "df['sensor_06'].fillna((df['sensor_06'].mean()), inplace=True)\n",
    "df['sensor_09'].fillna((df['sensor_09'].mean()), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa283bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:00:21.020028Z",
     "iopub.status.busy": "2023-04-02T00:00:21.019299Z",
     "iopub.status.idle": "2023-04-02T00:00:21.127377Z",
     "shell.execute_reply": "2023-04-02T00:00:21.126008Z"
    },
    "papermill": {
     "duration": 0.125808,
     "end_time": "2023-04-02T00:00:21.129744",
     "exception": false,
     "start_time": "2023-04-02T00:00:21.003936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tidy = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5befb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:00:21.158776Z",
     "iopub.status.busy": "2023-04-02T00:00:21.158415Z",
     "iopub.status.idle": "2023-04-02T00:00:21.256110Z",
     "shell.execute_reply": "2023-04-02T00:00:21.254556Z"
    },
    "papermill": {
     "duration": 0.114928,
     "end_time": "2023-04-02T00:00:21.258467",
     "exception": false,
     "start_time": "2023-04-02T00:00:21.143539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's convert the data type of timestamp column to datatime format\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "df_tidy['date'] = pd.to_datetime(df_tidy['timestamp'])\n",
    "del df_tidy['timestamp']\n",
    "\n",
    "df_tidy = df_tidy.set_index('date')\n",
    "df_tidy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8793f05f",
   "metadata": {
    "papermill": {
     "duration": 0.014885,
     "end_time": "2023-04-02T00:00:21.288449",
     "exception": false,
     "start_time": "2023-04-02T00:00:21.273564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aac67e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:00:21.321496Z",
     "iopub.status.busy": "2023-04-02T00:00:21.321020Z",
     "iopub.status.idle": "2023-04-02T00:00:21.374653Z",
     "shell.execute_reply": "2023-04-02T00:00:21.372942Z"
    },
    "papermill": {
     "duration": 0.073225,
     "end_time": "2023-04-02T00:00:21.377617",
     "exception": false,
     "start_time": "2023-04-02T00:00:21.304392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_tidy\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c45c97",
   "metadata": {
    "papermill": {
     "duration": 0.01599,
     "end_time": "2023-04-02T00:00:21.409717",
     "exception": false,
     "start_time": "2023-04-02T00:00:21.393727",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Quantative EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585a29d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:00:21.443719Z",
     "iopub.status.busy": "2023-04-02T00:00:21.443260Z",
     "iopub.status.idle": "2023-04-02T00:00:21.815606Z",
     "shell.execute_reply": "2023-04-02T00:00:21.813855Z"
    },
    "papermill": {
     "duration": 0.392032,
     "end_time": "2023-04-02T00:00:21.818480",
     "exception": false,
     "start_time": "2023-04-02T00:00:21.426448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quantative EDA\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2bc5e4",
   "metadata": {
    "papermill": {
     "duration": 0.016267,
     "end_time": "2023-04-02T00:00:21.850961",
     "exception": false,
     "start_time": "2023-04-02T00:00:21.834694",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Graphical EDA\n",
    "Let's visualize the sensor readings across the entire 52 sensors and mark the pump's broken state in red color on the same graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3109a8bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:00:21.885013Z",
     "iopub.status.busy": "2023-04-02T00:00:21.884629Z",
     "iopub.status.idle": "2023-04-02T00:00:34.586348Z",
     "shell.execute_reply": "2023-04-02T00:00:34.584975Z"
    },
    "papermill": {
     "duration": 12.721357,
     "end_time": "2023-04-02T00:00:34.588604",
     "exception": false,
     "start_time": "2023-04-02T00:00:21.867247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vizualize time series and the BROKEN state (red dots) in the same graph for each sensor\n",
    "import warnings\n",
    "# Extract the readings from BROKEN state and resample by daily average\n",
    "broken = df[df['machine_status']=='BROKEN']\n",
    "# Extract the names of the numerical columns\n",
    "df2 = df.drop(['machine_status'], axis=1)\n",
    "names=df2.columns\n",
    "# Plot time series for each sensor with BROKEN state marked with X in red color\n",
    "for name in names:\n",
    "    sns.set_context('talk')\n",
    "    _ = plt.figure(figsize=(18,3))\n",
    "    _ = plt.plot(broken[name], linestyle='none', marker='X', color='red', markersize=12)\n",
    "    _ = plt.plot(df[name], color='blue')\n",
    "    _ = plt.title(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c2f66b",
   "metadata": {
    "papermill": {
     "duration": 0.045836,
     "end_time": "2023-04-02T00:00:34.681055",
     "exception": false,
     "start_time": "2023-04-02T00:00:34.635219",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "346e017a",
   "metadata": {
    "papermill": {
     "duration": 0.049152,
     "end_time": "2023-04-02T00:00:34.776977",
     "exception": false,
     "start_time": "2023-04-02T00:00:34.727825",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Check Stationarity with rolling stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e8c43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:00:34.874883Z",
     "iopub.status.busy": "2023-04-02T00:00:34.874143Z",
     "iopub.status.idle": "2023-04-02T00:01:22.084167Z",
     "shell.execute_reply": "2023-04-02T00:01:22.082895Z"
    },
    "papermill": {
     "duration": 47.261312,
     "end_time": "2023-04-02T00:01:22.086460",
     "exception": false,
     "start_time": "2023-04-02T00:00:34.825148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resample the entire dataset by daily average\n",
    "rollmean = df.resample(rule='D').mean()\n",
    "rollstd = df.resample(rule='D').std()\n",
    "# Plot time series for each sensor with BROKEN state marked with X in red color\n",
    "for name in names:\n",
    "    _ = plt.figure(figsize=(18,3))\n",
    "    _ = plt.plot(df[name], color='blue', label='Original')\n",
    "    _ = plt.plot(rollmean[name], color='red', label='Rolling Mean')\n",
    "    _ = plt.plot(rollstd[name], color='black', label='Rolling Std' )\n",
    "    _ = plt.legend(loc='best')\n",
    "    _ = plt.title(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9123fae",
   "metadata": {
    "papermill": {
     "duration": 0.094173,
     "end_time": "2023-04-02T00:01:22.274201",
     "exception": false,
     "start_time": "2023-04-02T00:01:22.180028",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 4: Pre-processing and Feature Engineering\n",
    "In this step, I will scale the data and apply Principal Component Analysis (PCA) to extract the most important features to be further used in training models. It is computationally quite expensive to process the data of this size, (219521, 53), hence the reason for reducing the dimensionality with PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf11f7fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:01:22.464317Z",
     "iopub.status.busy": "2023-04-02T00:01:22.463675Z",
     "iopub.status.idle": "2023-04-02T00:01:23.081007Z",
     "shell.execute_reply": "2023-04-02T00:01:23.080194Z"
    },
    "papermill": {
     "duration": 0.715867,
     "end_time": "2023-04-02T00:01:23.083294",
     "exception": false,
     "start_time": "2023-04-02T00:01:22.367427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standardize/scale the dataset and apply PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# Extract the names of the numerical columns\n",
    "df2 = df.drop(['machine_status'], axis=1)\n",
    "names=df2.columns\n",
    "x = df[names]\n",
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "pipeline = make_pipeline(scaler, pca)\n",
    "pipeline.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a73670",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:01:23.292074Z",
     "iopub.status.busy": "2023-04-02T00:01:23.291675Z",
     "iopub.status.idle": "2023-04-02T00:01:23.817248Z",
     "shell.execute_reply": "2023-04-02T00:01:23.816034Z"
    },
    "papermill": {
     "duration": 0.621416,
     "end_time": "2023-04-02T00:01:23.819471",
     "exception": false,
     "start_time": "2023-04-02T00:01:23.198055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = range(pca.n_components_)\n",
    "_ = plt.figure(figsize=(22, 5))\n",
    "_ = plt.bar(features, pca.explained_variance_)\n",
    "_ = plt.xlabel('PCA feature')\n",
    "_ = plt.ylabel('Variance')\n",
    "_ = plt.xticks(features)\n",
    "_ = plt.title(\"Importance of the Principal Components based on inertia\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e8233d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:01:24.014342Z",
     "iopub.status.busy": "2023-04-02T00:01:24.013968Z",
     "iopub.status.idle": "2023-04-02T00:01:24.791308Z",
     "shell.execute_reply": "2023-04-02T00:01:24.790115Z"
    },
    "papermill": {
     "duration": 0.878449,
     "end_time": "2023-04-02T00:01:24.793685",
     "exception": false,
     "start_time": "2023-04-02T00:01:23.915236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate PCA with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['pc1', 'pc2'])\n",
    "df['pc1']=pd.Series(principalDf['pc1'].values, index=df.index)\n",
    "df['pc2']=pd.Series(principalDf['pc2'].values, index=df.index)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ce5eca",
   "metadata": {
    "papermill": {
     "duration": 0.093891,
     "end_time": "2023-04-02T00:01:24.981757",
     "exception": false,
     "start_time": "2023-04-02T00:01:24.887866",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Check stationarity with Dickey-Fuller Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaffca11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:01:25.174688Z",
     "iopub.status.busy": "2023-04-02T00:01:25.174216Z",
     "iopub.status.idle": "2023-04-02T00:02:20.618613Z",
     "shell.execute_reply": "2023-04-02T00:02:20.617618Z"
    },
    "papermill": {
     "duration": 55.674786,
     "end_time": "2023-04-02T00:02:20.752200",
     "exception": false,
     "start_time": "2023-04-02T00:01:25.077414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "# Run Augmented Dickey Fuller Test\n",
    "result = adfuller(principalDf['pc1'])\n",
    "# Print p-value\n",
    "print(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1133da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:02:20.948031Z",
     "iopub.status.busy": "2023-04-02T00:02:20.947710Z",
     "iopub.status.idle": "2023-04-02T00:03:15.609866Z",
     "shell.execute_reply": "2023-04-02T00:03:15.608997Z"
    },
    "papermill": {
     "duration": 54.894428,
     "end_time": "2023-04-02T00:03:15.744239",
     "exception": false,
     "start_time": "2023-04-02T00:02:20.849811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run Augmented Dickey Fuller Test\n",
    "result = adfuller(principalDf['pc2'])\n",
    "# Print p-value\n",
    "print(result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13938403",
   "metadata": {
    "papermill": {
     "duration": 0.094401,
     "end_time": "2023-04-02T00:03:15.935576",
     "exception": false,
     "start_time": "2023-04-02T00:03:15.841175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Check for Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52f504e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:03:16.127603Z",
     "iopub.status.busy": "2023-04-02T00:03:16.127216Z",
     "iopub.status.idle": "2023-04-02T00:03:23.050032Z",
     "shell.execute_reply": "2023-04-02T00:03:23.047941Z"
    },
    "papermill": {
     "duration": 7.021685,
     "end_time": "2023-04-02T00:03:23.052748",
     "exception": false,
     "start_time": "2023-04-02T00:03:16.031063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute change in daily mean \n",
    "pca1 = principalDf['pc1'].pct_change()\n",
    "# Compute autocorrelation\n",
    "autocorrelation = pca1.dropna().autocorr()\n",
    "print('Autocorrelation is: ', autocorrelation)\n",
    "\n",
    "# Plot ACF\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "plot_acf(pca1.dropna(), lags=20, alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e3c40",
   "metadata": {
    "papermill": {
     "duration": 0.097122,
     "end_time": "2023-04-02T00:03:23.246391",
     "exception": false,
     "start_time": "2023-04-02T00:03:23.149269",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 5: Modelling \n",
    "Base Model: Detect Outliers Using the Interquartile Range (IQR)\n",
    "Anomalies are defined as rare events that could be represented by the outliers in the data set. As an initial step, I want to apply a basic statistics technique to get the feel of the outliers present in this data set. Later, I will compare the results of the other models to the results from the Base Model for further model evaluation.\n",
    "\n",
    "0: normal\n",
    "\n",
    "1: anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd4fdef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:03:23.442264Z",
     "iopub.status.busy": "2023-04-02T00:03:23.441652Z",
     "iopub.status.idle": "2023-04-02T00:03:23.468546Z",
     "shell.execute_reply": "2023-04-02T00:03:23.467105Z"
    },
    "papermill": {
     "duration": 0.128481,
     "end_time": "2023-04-02T00:03:23.470807",
     "exception": false,
     "start_time": "2023-04-02T00:03:23.342326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# outlier_lower = Q1 - (1.5*IQR)\n",
    "# outlier_upper = Q3 + (1.5*IQR)\n",
    "# Calculate outlier bounds for pc1\n",
    "q1_pc1, q3_pc1 = df['pc1'].quantile([0.25, 0.75])\n",
    "iqr_pc1 = q3_pc1 - q1_pc1\n",
    "lower_pc1 = q1_pc1 - (1.5*iqr_pc1)\n",
    "upper_pc1 = q3_pc1 + (1.5*iqr_pc1)\n",
    "# Calculate outlier bounds for pc2\n",
    "q1_pc2, q3_pc2 = df['pc2'].quantile([0.25, 0.75])\n",
    "iqr_pc2 = q3_pc2 - q1_pc2\n",
    "lower_pc2 = q1_pc2 - (1.5*iqr_pc2)\n",
    "upper_pc2 = q3_pc2 + (1.5*iqr_pc2)\n",
    "\n",
    "df['anomaly_pc1'] = ((df['pc1']>upper_pc1) | (df['pc1']<lower_pc1)).astype('int')\n",
    "df['anomaly_pc2'] = ((df['pc2']>upper_pc2) | (df['pc2']<lower_pc2)).astype('int')\n",
    "print('Anomaly count with PC1\\n ',df['anomaly_pc1'].value_counts())\n",
    "print('\\nAnomaly count with PC2\\n ',df['anomaly_pc2'].value_counts())\n",
    "\n",
    "\n",
    "outliers_pc1 = df.loc[(df['pc1']>upper_pc1) | (df['pc1']<lower_pc1), 'pc1']\n",
    "outliers_pc2 = df.loc[(df['pc2']>upper_pc2) | (df['pc2']<lower_pc2), 'pc2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8bec2",
   "metadata": {
    "papermill": {
     "duration": 0.09679,
     "end_time": "2023-04-02T00:03:23.665329",
     "exception": false,
     "start_time": "2023-04-02T00:03:23.568539",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now I want to select the most important 3 features in the data set to detect anomalies in them. To find out these features, I will use Univariate feature selection technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dca75da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:03:23.860070Z",
     "iopub.status.busy": "2023-04-02T00:03:23.859167Z",
     "iopub.status.idle": "2023-04-02T00:03:24.507873Z",
     "shell.execute_reply": "2023-04-02T00:03:24.506987Z"
    },
    "papermill": {
     "duration": 0.748029,
     "end_time": "2023-04-02T00:03:24.510018",
     "exception": false,
     "start_time": "2023-04-02T00:03:23.761989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply SelectKBest class to extract the best 3 features - Univariate feature selection \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "x = df.drop(['machine_status', 'pc1', 'pc2', 'anomaly_pc1', 'anomaly_pc2'], axis=1)\n",
    "y = df['machine_status']\n",
    "scaler = MinMaxScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=3)\n",
    "fit = bestfeatures.fit(x_scaled, y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(x.columns)\n",
    "featureScores = pd.concat([dfcolumns, dfscores], axis=1)\n",
    "featureScores.columns = ['Feature', 'Score']\n",
    "print(featureScores.nlargest(3, 'Score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6377eaab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:03:24.732833Z",
     "iopub.status.busy": "2023-04-02T00:03:24.732421Z",
     "iopub.status.idle": "2023-04-02T00:03:26.084333Z",
     "shell.execute_reply": "2023-04-02T00:03:26.083249Z"
    },
    "papermill": {
     "duration": 1.449841,
     "end_time": "2023-04-02T00:03:26.086366",
     "exception": false,
     "start_time": "2023-04-02T00:03:24.636525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's plot the outliers from pc1 on top of the sensor_11 see where they occured in the time series\n",
    "a = df[df['anomaly_pc1'] == 1] #anomaly\n",
    "_ = plt.figure(figsize=(18,6))\n",
    "_ = plt.plot(df['sensor_11'], color='blue', label='Normal')\n",
    "_ = plt.plot(a['sensor_11'], linestyle='none', marker='X', color='red', markersize=12, label='Anomaly')\n",
    "_ = plt.xlabel('Date and Time')\n",
    "_ = plt.ylabel('Sensor Reading')\n",
    "_ = plt.title('Sensor_11 Anomalies')\n",
    "_ = plt.legend(loc='best')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d95f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:03:26.282464Z",
     "iopub.status.busy": "2023-04-02T00:03:26.282086Z",
     "iopub.status.idle": "2023-04-02T00:03:27.619435Z",
     "shell.execute_reply": "2023-04-02T00:03:27.618520Z"
    },
    "papermill": {
     "duration": 1.437695,
     "end_time": "2023-04-02T00:03:27.622054",
     "exception": false,
     "start_time": "2023-04-02T00:03:26.184359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's plot the outliers from pc2 on top of the sensor_00 see where they occured in the time series\n",
    "a = df[df['anomaly_pc1'] == 1] #anomaly\n",
    "_ = plt.figure(figsize=(18,6))\n",
    "_ = plt.plot(df['sensor_12'], color='blue', label='Normal')\n",
    "_ = plt.plot(a['sensor_12'], linestyle='none', marker='X', color='red', markersize=12, label='Anomaly')\n",
    "_ = plt.xlabel('Date and Time')\n",
    "_ = plt.ylabel('Sensor Reading')\n",
    "_ = plt.title('Sensor_12 Anomalies')\n",
    "_ = plt.legend(loc='best')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894fffcf",
   "metadata": {
    "papermill": {
     "duration": 0.096449,
     "end_time": "2023-04-02T00:03:27.816626",
     "exception": false,
     "start_time": "2023-04-02T00:03:27.720177",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model : K-means clustering\n",
    "Strategy:\n",
    "\n",
    "The underline assumption in the clustering based anomaly detection is that if we cluster the data, normal data will belong to clusters while anomalies will not belong to any clusters or belong to small clusters. We use the following steps to find and visualize anomalies.\n",
    "\n",
    "Calculate the distance between each point and its nearest centroid. The biggest distances are considered as anomaly.\n",
    "We use outliers_fraction to provide information to the algorithm about the proportion of the outliers present in our data set. Situations may vary from data set to data set. However, as a starting figure, I estimate outliers_fraction=0.14 (14% of df are outliers as depicted above).\n",
    "Calculate number_of_outliers using outliers_fraction.\n",
    "Set threshold as the minimum distance of these outliers.\n",
    "The anomaly result of anomaly1 contains the above method Cluster (0:normal, 1:anomaly).\n",
    "Visualize anomalies with cluster view.\n",
    "Visualize anomalies with Time Series view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fb9e43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:03:28.017867Z",
     "iopub.status.busy": "2023-04-02T00:03:28.016970Z",
     "iopub.status.idle": "2023-04-02T00:03:29.467936Z",
     "shell.execute_reply": "2023-04-02T00:03:29.466759Z"
    },
    "papermill": {
     "duration": 1.554994,
     "end_time": "2023-04-02T00:03:29.470739",
     "exception": false,
     "start_time": "2023-04-02T00:03:27.915745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.cluster import KMeans\n",
    "# I will start k-means clustering with k=2 as I already know that there are 3 classes of \"NORMAL\" vs \n",
    "# \"NOT NORMAL\" which are combination of BROKEN\" and\"RECOVERING\"\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans.fit(principalDf.values)\n",
    "labels = kmeans.predict(principalDf.values)\n",
    "unique_elements, counts_elements = np.unique(labels, return_counts=True)\n",
    "clusters = np.asarray((unique_elements, counts_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d756ebba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:03:29.685483Z",
     "iopub.status.busy": "2023-04-02T00:03:29.685144Z",
     "iopub.status.idle": "2023-04-02T00:03:29.848466Z",
     "shell.execute_reply": "2023-04-02T00:03:29.847650Z"
    },
    "papermill": {
     "duration": 0.264404,
     "end_time": "2023-04-02T00:03:29.850498",
     "exception": false,
     "start_time": "2023-04-02T00:03:29.586094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize = (9, 7))\n",
    "_ = plt.bar(clusters[0], clusters[1], tick_label=clusters[0])\n",
    "_ = plt.xlabel('Clusters')\n",
    "_ = plt.ylabel('Number of points')\n",
    "_ = plt.title('Number of points in each cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5460a5ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:03:30.051543Z",
     "iopub.status.busy": "2023-04-02T00:03:30.051173Z",
     "iopub.status.idle": "2023-04-02T00:03:33.859753Z",
     "shell.execute_reply": "2023-04-02T00:03:33.858036Z"
    },
    "papermill": {
     "duration": 3.910223,
     "end_time": "2023-04-02T00:03:33.862295",
     "exception": false,
     "start_time": "2023-04-02T00:03:29.952072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(9,7))\n",
    "_ = plt.scatter(principalDf['pc1'], principalDf['pc2'], c=labels)\n",
    "_ = plt.xlabel('pc1')\n",
    "_ = plt.ylabel('pc2')\n",
    "_ = plt.title('K-means of clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24901f07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:03:34.071040Z",
     "iopub.status.busy": "2023-04-02T00:03:34.070674Z",
     "iopub.status.idle": "2023-04-02T00:03:34.077755Z",
     "shell.execute_reply": "2023-04-02T00:03:34.076571Z"
    },
    "papermill": {
     "duration": 0.115443,
     "end_time": "2023-04-02T00:03:34.079774",
     "exception": false,
     "start_time": "2023-04-02T00:03:33.964331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write a function that calculates distance between each point and the centroid of the closest cluster\n",
    "def getDistanceByPoint(data, model):\n",
    "    \"\"\" Function that calculates the distance between a point and centroid of a cluster, \n",
    "            returns the distances in pandas series\"\"\"\n",
    "    distance = []\n",
    "    for i in range(0,len(data)):\n",
    "        Xa = np.array(data.loc[i])\n",
    "        Xb = model.cluster_centers_[model.labels_[i]-1]\n",
    "        distance.append(np.linalg.norm(Xa-Xb))\n",
    "    return pd.Series(distance, index=data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f07ab18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:03:34.283969Z",
     "iopub.status.busy": "2023-04-02T00:03:34.282703Z",
     "iopub.status.idle": "2023-04-02T00:03:54.854304Z",
     "shell.execute_reply": "2023-04-02T00:03:54.853523Z"
    },
    "papermill": {
     "duration": 20.674384,
     "end_time": "2023-04-02T00:03:54.856732",
     "exception": false,
     "start_time": "2023-04-02T00:03:34.182348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assume that 13% of the entire data set are anomalies \n",
    "outliers_fraction = 0.13\n",
    "# get the distance between each point and its nearest centroid. The biggest distances are considered as anomaly\n",
    "distance = getDistanceByPoint(principalDf, kmeans)\n",
    "# number of observations that equate to the 13% of the entire data set\n",
    "number_of_outliers = int(outliers_fraction*len(distance))\n",
    "# Take the minimum of the largest 13% of the distances as the threshold\n",
    "threshold = distance.nlargest(number_of_outliers).min()\n",
    "# anomaly1 contain the anomaly result of the above method Cluster (0:normal, 1:anomaly) \n",
    "principalDf['anomaly1'] = (distance >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18022da5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:03:55.063332Z",
     "iopub.status.busy": "2023-04-02T00:03:55.062704Z",
     "iopub.status.idle": "2023-04-02T00:03:55.072378Z",
     "shell.execute_reply": "2023-04-02T00:03:55.071652Z"
    },
    "papermill": {
     "duration": 0.115385,
     "end_time": "2023-04-02T00:03:55.074080",
     "exception": false,
     "start_time": "2023-04-02T00:03:54.958695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "principalDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d86711e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:03:55.278491Z",
     "iopub.status.busy": "2023-04-02T00:03:55.277899Z",
     "iopub.status.idle": "2023-04-02T00:03:55.286848Z",
     "shell.execute_reply": "2023-04-02T00:03:55.286080Z"
    },
    "papermill": {
     "duration": 0.113042,
     "end_time": "2023-04-02T00:03:55.288621",
     "exception": false,
     "start_time": "2023-04-02T00:03:55.175579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "principalDf['anomaly1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9a729f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:03:55.488112Z",
     "iopub.status.busy": "2023-04-02T00:03:55.487125Z",
     "iopub.status.idle": "2023-04-02T00:03:56.822040Z",
     "shell.execute_reply": "2023-04-02T00:03:56.820702Z"
    },
    "papermill": {
     "duration": 1.436454,
     "end_time": "2023-04-02T00:03:56.824618",
     "exception": false,
     "start_time": "2023-04-02T00:03:55.388164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['anomaly1'] = pd.Series(principalDf['anomaly1'].values, index=df.index)\n",
    "a = df[df['anomaly1'] == 1] #anomaly\n",
    "_ = plt.figure(figsize=(18,6))\n",
    "_ = plt.plot(df['sensor_11'], color='blue', label='Normal')\n",
    "_ = plt.plot(a['sensor_11'], linestyle='none', marker='X', color='red', markersize=12, label='Anomaly')\n",
    "_ = plt.xlabel('Date and Time')\n",
    "_ = plt.ylabel('Sensor Reading')\n",
    "_ = plt.title('Sensor_11 Anomalies')\n",
    "_ = plt.legend(loc='best')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047cc1f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:03:57.032062Z",
     "iopub.status.busy": "2023-04-02T00:03:57.031702Z",
     "iopub.status.idle": "2023-04-02T00:03:58.383295Z",
     "shell.execute_reply": "2023-04-02T00:03:58.381961Z"
    },
    "papermill": {
     "duration": 1.457618,
     "end_time": "2023-04-02T00:03:58.385393",
     "exception": false,
     "start_time": "2023-04-02T00:03:56.927775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = df[df['anomaly1'] == 1] #anomaly\n",
    "_ = plt.figure(figsize=(18,6))\n",
    "_ = plt.plot(df['sensor_12'], color='blue', label='Normal')\n",
    "_ = plt.plot(a['sensor_12'], linestyle='none', marker='X', color='red', markersize=12, label='Anomaly')\n",
    "_ = plt.xlabel('Date and Time')\n",
    "_ = plt.ylabel('Sensor Reading')\n",
    "_ = plt.title('Sensor_12 Anomalies')\n",
    "_ = plt.legend(loc='best')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159f32b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:03:58.589404Z",
     "iopub.status.busy": "2023-04-02T00:03:58.588830Z",
     "iopub.status.idle": "2023-04-02T00:03:59.127338Z",
     "shell.execute_reply": "2023-04-02T00:03:59.126055Z"
    },
    "papermill": {
     "duration": 0.642438,
     "end_time": "2023-04-02T00:03:59.129519",
     "exception": false,
     "start_time": "2023-04-02T00:03:58.487081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = df[df['anomaly1'] == 1] #anomaly\n",
    "_ = plt.figure(figsize=(18,6))\n",
    "_ = plt.plot(df['sensor_04'], color='blue', label='Normal')\n",
    "_ = plt.plot(a['sensor_04'], linestyle='none', marker='X', color='red', markersize=12, label='Anomaly')\n",
    "_ = plt.xlabel('Date and Time')\n",
    "_ = plt.ylabel('Sensor Reading')\n",
    "_ = plt.title('Sensor_04 Anomalies')\n",
    "_ = plt.legend(loc='best')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881b79db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:03:59.346111Z",
     "iopub.status.busy": "2023-04-02T00:03:59.345694Z",
     "iopub.status.idle": "2023-04-02T00:03:59.363086Z",
     "shell.execute_reply": "2023-04-02T00:03:59.362064Z"
    },
    "papermill": {
     "duration": 0.127284,
     "end_time": "2023-04-02T00:03:59.365756",
     "exception": false,
     "start_time": "2023-04-02T00:03:59.238472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df['anomaly1']==1]['machine_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6195c76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:03:59.581656Z",
     "iopub.status.busy": "2023-04-02T00:03:59.580622Z",
     "iopub.status.idle": "2023-04-02T00:03:59.602630Z",
     "shell.execute_reply": "2023-04-02T00:03:59.601214Z"
    },
    "papermill": {
     "duration": 0.133339,
     "end_time": "2023-04-02T00:03:59.604731",
     "exception": false,
     "start_time": "2023-04-02T00:03:59.471392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['machine_status'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 256.790106,
   "end_time": "2023-04-02T00:04:02.969130",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-01T23:59:46.179024",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
